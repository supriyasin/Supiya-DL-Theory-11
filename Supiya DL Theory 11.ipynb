{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e79822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.7359083570635414\n"
     ]
    }
   ],
   "source": [
    "#1. Write the Python code to implement a single neuron.\n",
    "\n",
    "#Here's an example of Python code that implements a single neuron:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.weights = np.random.randn(num_inputs)  # Initialize weights randomly\n",
    "        self.bias = np.random.randn()  # Initialize bias randomly\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Weighted sum of inputs and bias\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "        # Activation function (in this case, using a sigmoid function)\n",
    "        activation = self._sigmoid(weighted_sum)\n",
    "\n",
    "        return activation\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage\n",
    "inputs = np.array([0.5, 0.3, 0.2])  # Input values\n",
    "neuron = Neuron(len(inputs))  # Create a neuron with the appropriate number of inputs\n",
    "output = neuron.forward(inputs)  # Compute the output of the neuron\n",
    "\n",
    "print(\"Output:\", output)\n",
    "\n",
    "\"\"\"In this code, we define a Neuron class that represents a single neuron. The __init__ method initializes the weights\n",
    "   and bias randomly. The forward method takes an input vector, performs a weighted sum of the inputs and bias, and\n",
    "   applies an activation function (in this case, the sigmoid function). The _sigmoid method is a helper function that \n",
    "   implements the sigmoid activation function.\n",
    "\n",
    "   We then create an instance of the Neuron class, passing the number of inputs to the constructor. Finally, we call \n",
    "   the forward method with an input vector to obtain the output of the neuron, and print the result.\n",
    "\n",
    "   Note that this is a basic implementation of a single neuron and does not include features like training or optimization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4835e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0 2 0 4 0]\n"
     ]
    }
   ],
   "source": [
    "#2. Write the Python code to implement ReLU.\n",
    "\n",
    "#Here's an example of Python code that implements the Rectified Linear Unit (ReLU) activation function:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Example usage\n",
    "input_value = np.array([-1, 2, -3, 4, -5])\n",
    "output = relu(input_value)\n",
    "\n",
    "print(\"Output:\", output)\n",
    "\n",
    "\"\"\"In this code, we define a function called relu that implements the ReLU activation function. The relu function \n",
    "   takes an input x and returns the element-wise maximum of x and 0, effectively replacing negative values with zeros.\n",
    "\n",
    "   We then create an example input array, input_value, and apply the ReLU activation function to it using the relu\n",
    "   function. The resulting output is stored in the output variable, which we print to the console.\n",
    "\n",
    "   Note that this code assumes the use of NumPy for efficient element-wise operations. If you haven't already, make\n",
    "   sure to install NumPy using pip install numpy before running the code.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df781cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[ -0.85912004  -5.6079855 ]\n",
      " [ -2.34975023 -12.04977258]]\n"
     ]
    }
   ],
   "source": [
    "#3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "#Here's an example of Python code that implements a dense layer using matrix multiplication:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)  # Initialize weights randomly\n",
    "        self.bias = np.random.randn(output_size)  # Initialize bias randomly\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Perform matrix multiplication between inputs and weights\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "# Example usage\n",
    "inputs = np.array([[1, 2, 3], [4, 5, 6]])  # Input values (2 examples with 3 features each)\n",
    "dense_layer = DenseLayer(input_size=3, output_size=2)  # Create a dense layer with the appropriate input and output sizes\n",
    "output = dense_layer.forward(inputs)  # Compute the output of the dense layer\n",
    "\n",
    "print(\"Output:\", output)\n",
    "\n",
    "\"\"\"In this code, we define a DenseLayer class that represents a dense layer in a neural network. The __init__ method \n",
    "   initializes the weights and bias randomly. The forward method takes an input matrix and performs matrix multiplication \n",
    "   between the inputs and weights, adds the bias term, and returns the result.\n",
    " \n",
    "   We then create an instance of the DenseLayer class, passing the input size and output size to the constructor.\n",
    "   The input matrix, inputs, contains multiple examples, where each example is a row and each column represents a\n",
    "   feature. Finally, we call the forward method with the input matrix to obtain the output of the dense layer, and \n",
    "   print the result.\n",
    "\n",
    "   Note that in this example, we assume that the inputs are organized in a matrix where each row represents an example\n",
    "   and each column represents a feature. The weights matrix is initialized with the shape (input_size, output_size), where \n",
    "   input_size is the number of features in the input and output_size is the desired output size. The output of the dense\n",
    "   layer will have the shape (num_examples, output_size), where num_examples is the number of examples in the input matrix.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built \n",
    "into Python).\n",
    "\n",
    "#Here's an example of Python code that implements a dense layer using plain Python with list comprehensions:\n",
    "\n",
    "import random\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = [[random.random() for _ in range(output_size)] for _ in range(input_size)]\n",
    "        self.bias = [random.random() for _ in range(output_size)]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = []\n",
    "        for example in inputs:\n",
    "            weighted_sum.append([sum(example[i] * self.weights[j][i] for i in range(len(example))) + self.bias[j]\n",
    "                                 for j in range(len(self.bias))])\n",
    "\n",
    "        return weighted_sum\n",
    "\n",
    "# Example usage\n",
    "inputs = [[1, 2, 3], [4, 5, 6]]  # Input values (2 examples with 3 features each)\n",
    "dense_layer = DenseLayer(input_size=3, output_size=2)  # Create a dense layer with the appropriate input and output sizes\n",
    "output = dense_layer.forward(inputs)  # Compute the output of the dense layer\n",
    "\n",
    "print(\"Output:\", output)\n",
    "\n",
    "\n",
    "\"\"\"In this code, we define a DenseLayer class that represents a dense layer in a neural network. The __init__ method \n",
    "   initializes the weights and bias using list comprehensions and the random.random() function. The weights are \n",
    "   represented as a list of lists, where each inner list corresponds to a row of weights, and the bias is a list \n",
    "   of bias values.\n",
    "\n",
    "   The forward method takes an input matrix, inputs, and iterates over each example using a for loop. For each example, \n",
    "   it computes the weighted sum by multiplying the input values with the corresponding weights, summing them up, and\n",
    "   adding the bias. The result is stored in the weighted_sum list.\n",
    "\n",
    "   We then create an instance of the DenseLayer class, passing the input size and output size to the constructor. \n",
    "   The input matrix, inputs, contains multiple examples, where each example is a list and each element represents a\n",
    "   feature. Finally, we call the forward method with the input matrix to obtain the output of the dense layer, and \n",
    "   print the result.\n",
    "\n",
    "   Note that in this plain Python implementation, we are using nested list comprehensions and manual iteration to \n",
    "   perform the matrix multiplication and summation. While this approach works for small-scale scenarios, it may not \n",
    "   be as efficient as using optimized numerical libraries like NumPy for larger-scale applications.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0bbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What is the “hidden size” of a layer?\n",
    "\n",
    "\"\"\"The \"hidden size\" of a layer refers to the number of neurons or units within that layer. In a neural network, a layer\n",
    "   consists of a group of neurons that collectively perform computations on the input data.\n",
    "\n",
    "   Each neuron in a layer takes inputs, applies weights and biases, and produces an output. The hidden size of a layer\n",
    "   determines the number of neurons in that layer, which in turn affects the expressive power and complexity of the \n",
    "   layer's computations.\n",
    "\n",
    "   The hidden size of a layer is an important parameter in neural network architecture design. It impacts the capacity \n",
    "   of the network to learn and represent complex patterns in the data. A larger hidden size allows for more complex and\n",
    "   expressive computations but may also increase the model's computational and memory requirements. On the other hand,\n",
    "   a smaller hidden size reduces the complexity but may limit the model's ability to capture intricate patterns.\n",
    "\n",
    "   The choice of hidden size depends on various factors, such as the complexity of the problem being solved, the size\n",
    "   of the input data, and the available computational resources. It is often determined through experimentation and \n",
    "   iterative refinement to achieve a balance between model complexity and generalization capability.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4823928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. What does the t method do in PyTorch?\n",
    "\n",
    "\"\"\"In PyTorch, the t method is used to transpose a tensor. Transposing a tensor swaps its dimensions, effectively \n",
    "   rearranging the shape of the tensor.\n",
    "\n",
    "   The t method is available on PyTorch tensors and can be invoked by calling it on the tensor object.\"\"\"\n",
    "\n",
    "#Here's an example:\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "y = x.t()\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(\"Transposed tensor:\")\n",
    "print(y)\n",
    "\n",
    "\"\"\"In the example, we define a 2D tensor x with dimensions (2, 3). By calling the t method on x and storing the result\n",
    "   in y, we obtain a transposed tensor with dimensions (3, 2), where the rows of x become columns in y and vice versa.\n",
    "\n",
    "   The t method is particularly useful for reshaping tensors when their dimensions need to be reorganized, such as when\n",
    "   preparing inputs for matrix operations or rearranging the dimensions to match the required input shape for a particular\n",
    "   neural network layer.\n",
    "\n",
    "   It's important to note that the t method creates a new tensor with the transposed shape and does not modify the \n",
    "   original tensor in-place. If you wish to modify the tensor in-place, you can use the torch.transpose function instead.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "\"\"\"Matrix multiplication implemented in plain Python can be slow compared to optimized numerical libraries like NumPy \n",
    "   or dedicated linear algebra libraries for a few reasons:\n",
    "\n",
    "   1. Lack of vectorization: In plain Python, you typically perform element-wise computations using loops, which can \n",
    "      be inefficient for large-scale matrix operations. Vectorization, on the other hand, allows operations to be\n",
    "      executed in parallel, leveraging highly optimized, low-level operations implemented in libraries like NumPy. \n",
    "      These libraries are designed to efficiently handle matrix operations and take advantage of hardware optimizations, \n",
    "      such as SIMD (Single Instruction, Multiple Data) instructions.\n",
    "\n",
    "   2. Interpretation overhead: Python is an interpreted language, which means that each line of code is interpreted\n",
    "      and executed at runtime. This introduces additional overhead compared to compiled languages like C or C++. Numerical\n",
    "      libraries like NumPy, on the other hand, often use compiled code and low-level optimizations, allowing for faster \n",
    "      execution.\n",
    "\n",
    "   3. Memory management: In plain Python, lists or nested lists are commonly used to represent matrices, which introduces \n",
    "      additional memory overhead and inefficient memory access patterns. In contrast, optimized libraries like NumPy\n",
    "      use contiguous memory blocks and utilize efficient memory access patterns, resulting in improved performance.\n",
    "\n",
    "   4. Function call overhead: In plain Python, function calls are relatively expensive compared to compiled languages. \n",
    "      Matrix multiplication involves multiple function calls for indexing, multiplication, and summation operations, \n",
    "      leading to performance degradation compared to optimized implementations.\n",
    "\n",
    "  By using optimized numerical libraries like NumPy, TensorFlow, or PyTorch, you can leverage their efficient \n",
    "  implementations, native support for vectorization, and hardware optimizations. These libraries provide highly \n",
    "  optimized matrix multiplication routines, enabling faster computation and improved performance for numerical operations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99826a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. In matmul, why is ac==br?\n",
    "\n",
    "\"\"\"In the context of matrix multiplication, the condition ac == br refers to the compatibility condition that must \n",
    "   be satisfied for two matrices to be multiplied together using the matmul operation.\n",
    "\n",
    "   In matrix multiplication, you multiply an m × n matrix A by an n × p matrix B to obtain an m × p matrix C. \n",
    "   The compatibility condition states that the number of columns in matrix A (n) must be equal to the number of \n",
    "   rows in matrix B (n) for the multiplication to be defined.\n",
    "\n",
    "   Let's break it down:\n",
    "\n",
    "   . Matrix A has dimensions m × n, where m is the number of rows and n is the number of columns.\n",
    "   . Matrix B has dimensions n × p, where n is the number of rows and p is the number of columns.\n",
    "   \n",
    "   To compute the matrix product C = A @ B using the matmul operation, the number of columns in matrix A (n) must be \n",
    "   equal to the number of rows in matrix B (n), as denoted by ac == br. This condition ensures that the inner dimensions \n",
    "   match, allowing for valid matrix multiplication.\n",
    "\n",
    "   If ac is not equal to br, the matrix multiplication operation is not defined, and you will encounter a dimension \n",
    "   mismatch error.\n",
    "\n",
    "   Therefore, the condition ac == br serves as a compatibility check, ensuring that the matrices being multiplied\n",
    "   together have compatible dimensions, leading to a valid matrix multiplication operation and resulting in a well-defined\n",
    "   output matrix.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c30497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "\"\"\"In Jupyter Notebook, you can measure the time taken for a single cell to execute by using the %%time magic command \n",
    "   or the %%timeit magic command.\n",
    "\n",
    "   1. %%time: This magic command measures the execution time of a single cell and displays the elapsed time in seconds.\n",
    "   \n",
    "   Here's an example of how to use %%time:\n",
    "   \n",
    "   %%time\n",
    "\n",
    "   # Your code here\n",
    "   \n",
    "   Simply place %%time at the beginning of the cell where you want to measure the execution time. When you run the cell, \n",
    "   the elapsed time will be displayed in the output.\n",
    "\n",
    "   2. %%timeit: This magic command executes the cell multiple times and provides the average execution time. \n",
    "      It is useful for measuring the performance of a code snippet over multiple runs to get more accurate timing results.\n",
    "      \n",
    "   Here's an example of how to use %%timeit:\n",
    "   \n",
    "   %%timeit\n",
    "\n",
    "  # Your code here\n",
    "\n",
    "  Similarly, place %%timeit at the beginning of the cell. When you run the cell, it will execute multiple times, \n",
    "  and the average execution time will be displayed in the output.\n",
    "\n",
    "  Using either of these magic commands allows you to conveniently measure the execution time of a single cell in \n",
    "  Jupyter Notebook, helping you understand the performance characteristics of your code and identify areas for optimization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce157d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. What is elementwise arithmetic?\n",
    "\n",
    "\"\"\"Elementwise arithmetic refers to performing arithmetic operations on corresponding elements of two or more arrays \n",
    "   or vectors. In this type of operation, the arithmetic operation is applied to each element individually, without \n",
    "   any dependency on neighboring elements.\n",
    "\n",
    "   For example, given two arrays A and B of the same shape, elementwise addition would involve adding the elements \n",
    "   at corresponding positions:\n",
    "   \n",
    "   A = [1, 2, 3]\n",
    "   B = [4, 5, 6]\n",
    "\n",
    "   Result = A + B = [1 + 4, 2 + 5, 3 + 6] = [5, 7, 9]\n",
    "   \n",
    "   Similarly, elementwise subtraction, multiplication, and division can be performed by applying the respective \n",
    "   arithmetic operations to the corresponding elements of the arrays.\n",
    "\n",
    "   Elementwise arithmetic is a fundamental concept in many numerical computing libraries, such as NumPy, PyTorch,\n",
    "   and TensorFlow. These libraries provide optimized implementations for performing elementwise operations efficiently \n",
    "   on large arrays or tensors. Elementwise arithmetic is often used in various mathematical and scientific computations, \n",
    "   such as vector operations, matrix operations, and neural network calculations, where elementwise operations are applied \n",
    "   to arrays or tensors element by element to compute the desired result.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n",
    "\n",
    "\"\"\"To test whether every element of tensor a is greater than the corresponding element of tensor b in PyTorch,\n",
    "   you can use the torch.all() function along with the > operator.\n",
    "\n",
    "   Here's an example code snippet:\n",
    "   \n",
    "   import torch\n",
    "\n",
    "   a = torch.tensor([1, 2, 3])\n",
    "   b = torch.tensor([0, 1, 2])\n",
    "\n",
    "   result = torch.all(a > b)\n",
    "\n",
    "   print(result)\n",
    "   \n",
    "   In this code, we define two tensors a and b with the same shape. We then perform the elementwise comparison a > b,\n",
    "   which results in a Boolean tensor where each element represents whether the corresponding element of a is greater \n",
    "   than the corresponding element of b.\n",
    "\n",
    "   Finally, we use the torch.all() function to check if all elements of the resulting tensor are True. If all elements \n",
    "   are True, the function will return True, indicating that every element of a is indeed greater than the corresponding\n",
    "   element of b. Otherwise, if any element is False, the function will return False.\n",
    "\n",
    "   Note that this code assumes that both a and b are tensors of the same shape. If the tensors have different shapes, \n",
    "   the elementwise comparison may result in broadcasting, which could yield unexpected results. It's important to ensure \n",
    "   that the shapes of a and b are compatible for elementwise comparison.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "\"\"\"In the context of tensors, a rank-0 tensor, also known as a scalar tensor or 0-dimensional tensor, represents a single \n",
    "   value without any dimensions or shape. It is the simplest form of a tensor and can be thought of as a single element \n",
    "   of a tensor.\n",
    "\n",
    "   In PyTorch, a rank-0 tensor is created using the torch.tensor() function by passing a single value as an argument. \n",
    "   Here's an example:\n",
    "\n",
    "   import torch\n",
    "\n",
    "  scalar_tensor = torch.tensor(5)  # Creating a rank-0 tensor with the value 5\n",
    "  \n",
    "  To convert a rank-0 tensor to a plain Python data type, you can use the .item() method. This method extracts the \n",
    "  value from the rank-0 tensor and returns it as a plain Python scalar. Here's an example:\n",
    "  \n",
    "  import torch\n",
    "\n",
    "  scalar_tensor = torch.tensor(5)  # Creating a rank-0 tensor with the value 5\n",
    "\n",
    "  scalar_value = scalar_tensor.item()  # Converting the rank-0 tensor to a plain Python data type\n",
    "\n",
    "  print(scalar_value)  # Output: 5\n",
    "  print(type(scalar_value))  # Output: <class 'int'>\n",
    "  \n",
    "  In this example, we create a rank-0 tensor scalar_tensor with the value 5. We then use the .item() method to extract \n",
    "  the value from the tensor and store it in the scalar_value variable. Finally, we print the scalar_value, which outputs 5, \n",
    "  and verify its data type using the type() function, which shows that it is a plain Python int.\n",
    "\n",
    "  The .item() method can only be used on rank-0 tensors since it extracts a single value. If you try to use it on tensors\n",
    "  with higher ranks (dimensions), you will get an error.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "\"\"\"Elementwise arithmetic alone does not directly speed up matrix multiplication (matmul). However, elementwise \n",
    "   arithmetic operations are often involved as part of the broader process of matrix multiplication, and optimized \n",
    "   implementations of elementwise operations can indirectly improve the performance of matrix multiplication.\n",
    "\n",
    "   Matrix multiplication involves a combination of elementwise multiplication and summation operations. Given two \n",
    "   matrices A and B, the resulting matrix C is computed by taking dot products of rows in A and columns in B. Each \n",
    "   element of C is calculated by multiplying corresponding elements from the respective rows and columns and summing them up.\n",
    "\n",
    "   Optimized implementations of elementwise arithmetic operations can contribute to speeding up matrix multiplication in \n",
    "   the following ways:\n",
    "   \n",
    "   1. Vectorization: Elementwise operations, such as elementwise multiplication or addition, can be performed in parallel \n",
    "      using vectorized operations provided by optimized numerical libraries like NumPy or dedicated linear algebra \n",
    "      libraries. These libraries leverage low-level optimizations, such as SIMD instructions, to efficiently perform \n",
    "      computations on contiguous blocks of memory, resulting in faster execution.\n",
    "\n",
    "   2. Efficient memory access: Optimized implementations of elementwise operations utilize efficient memory access \n",
    "      patterns, such as loop unrolling or blocking, to minimize cache misses and maximize data locality. This can \n",
    "      improve performance when accessing elements from matrices during the matrix multiplication process.\n",
    "\n",
    "   3. Optimized backend implementations: Numerical libraries like NumPy often have optimized backend implementations  \n",
    "      for elementwise operations that leverage highly optimized C/C++ or Fortran code, taking advantage of hardware-specific \n",
    "      optimizations and low-level operations. These optimized implementations can significantly speed up the execution of \n",
    "      elementwise operations, indirectly improving the overall performance of matrix multiplication.\n",
    "\n",
    "  By efficiently performing elementwise arithmetic operations during the matrix multiplication process, optimized libraries\n",
    "  can minimize unnecessary overhead and improve the overall computation time, resulting in faster matrix multiplication.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf47431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. What are the broadcasting rules?\n",
    "\n",
    "\"\"\"Broadcasting rules are a set of rules that define how elementwise operations are applied to arrays or tensors with \n",
    "   different shapes in order to make the shapes compatible. Broadcasting allows for implicit expansion of smaller \n",
    "   arrays to match the shape of larger arrays, enabling elementwise operations to be performed efficiently.\n",
    "\n",
    "   The broadcasting rules in NumPy and similar libraries are as follows:\n",
    "\n",
    "   1. Rule 1: Dimensions of size 1: If two arrays have different numbers of dimensions, the array with fewer dimensions\n",
    "      is padded with dimensions of size 1 on its left until the numbers of dimensions match.\n",
    "\n",
    "   2. Rule 2: Dimensions of size > 1: If the shape of any dimension in the two arrays is not equal and not 1, an error \n",
    "      is raised, indicating that the arrays are not compatible for broadcasting.\n",
    "\n",
    "   3. Rule 3: Compatible dimensions: If the shapes of the two arrays are unequal along any dimension, and neither is\n",
    "      equal to 1, then broadcasting is not possible, and an error is raised.\n",
    "\n",
    "   4. Rule 4: Resulting shape: The resulting shape of the operation is determined by taking the maximum size along each \n",
    "      dimension from the input arrays.\n",
    "\n",
    "   To illustrate these rules, consider the following example:\n",
    "   \n",
    "   import numpy as np\n",
    "\n",
    "   A = np.array([[1, 2, 3]])  # Shape: (1, 3)\n",
    "   B = np.array([4])  # Shape: (1,)\n",
    "   C = np.array([5, 6, 7])  # Shape: (3,)\n",
    "\n",
    "  D = A + B  # Broadcasting occurs: B is expanded to match A's shape (1, 3)\n",
    "  E = B + C  # Broadcasting occurs: B is expanded to match C's shape (3,)\n",
    "\n",
    "  print(D)  # Output: [[5, 6, 7]]\n",
    "  print(E)  # Output: [[9, 10, 11]]\n",
    "  \n",
    "  In this example, we have arrays A, B, and C with different shapes. Through broadcasting, we can perform elementwise\n",
    "  addition on these arrays. B is expanded to match the shape of A and C by adding dimensions of size 1. As a result, \n",
    "  the elementwise addition operation can be performed efficiently, producing arrays D and E with matching shapes.\n",
    "\n",
    "  Broadcasting allows for the efficient computation of elementwise operations on arrays of different shapes without \n",
    "  explicitly replicating or resizing the arrays. It simplifies the code and improves performance by avoiding unnecessary\n",
    "  memory duplication.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "\"\"\"In PyTorch, the expand_as method is used to expand the size of a tensor to match the size of another tensor. \n",
    "   It allows for aligning the dimensions of one tensor with the dimensions of another tensor, enabling operations\n",
    "   like broadcasting to be applied consistently.\n",
    "\n",
    "   The expand_as method takes a single argument, which is the tensor to be expanded. It returns a new tensor with \n",
    "   the same data as the original tensor but expanded to match the size of the input tensor.\n",
    "\n",
    "   Here's an example that demonstrates how expand_as can be used to match the results of broadcasting:\n",
    "   \n",
    "   import torch\n",
    "\n",
    "   A = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)\n",
    "   B = torch.tensor([4])  # Shape: (1,)\n",
    "   C = torch.tensor([5, 6, 7])  # Shape: (3,)\n",
    "\n",
    "   B_expanded = B.expand_as(A)  # Expanding B to match the shape of A\n",
    "   D = A + B_expanded\n",
    "\n",
    "  print(D)  # Output: tensor([[5, 6, 7]])\n",
    "  \n",
    "  In this example, we have tensors A, B, and C with different shapes. To match the results of broadcasting, we use the\n",
    "  expand_as method to expand tensor B to match the shape of tensor A. The resulting expanded tensor B_expanded has the \n",
    "  shape (1, 3), which matches the shape of A. We then perform elementwise addition of A and B_expanded to obtain tensor D, \n",
    "  which matches the result achieved through broadcasting.\n",
    "\n",
    "  By using expand_as, you can ensure that tensors have compatible shapes for elementwise operations or other computations,\n",
    "  allowing for consistent and expected results, even when the original tensors have different shapes.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
